# --- Project settings ---
PROJECT_NAME=olm-api

# --- API server settings ---
HOST_BIND_IP=127.0.0.1
NUM_OF_UVICORN_WORKERS=10
HOST_PORT=8000
DEV_PORT=8001
TEST_PORT=8002
API_LOGGING_ENABLED=false

# --- Ollama settings ---
BUILT_IN_OLLAMA_MODELS=qwen3:0.6b
OLLAMA_HOST=http://ollama:11434
# OLLAMA_HOST=http://host.docker.internal:11434
OLLAMA_CONTEXT_LENGTH=4096
OLLAMA_MAX_LOADED_MODELS=2
CONCURRENT_REQUEST_LIMIT=4
OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_QUEUE=256
OLLAMA_FLASH_ATTENTION=true
OLLAMA_KEEP_ALIVE=-1
OLLAMA_GPU_OVERHEAD=1024
OLLAMA_KV_CACHE_TYPE=q8_0

# export BUILT_IN_OLLAMA_MODELS=qwen3:0.6b
# export OLLAMA_CONTEXT_LENGTH=8192
# export OLLAMA_MAX_LOADED_MODELS=1
# export CONCURRENT_REQUEST_LIMIT=2
# export OLLAMA_NUM_PARALLEL=2
# export OLLAMA_MAX_QUEUE=256
# export OLLAMA_FLASH_ATTENTION=true
# export OLLAMA_KEEP_ALIVE=-1
# export OLLAMA_GPU_OVERHEAD=512
# export OLLAMA_KV_CACHE_TYPE=q8_0
