# --- Ollama settings ---
# BUILT_IN_OLLAMA_MODEL: Dockerイメージに組み込まれ、常に利用可能であるべきベースモデル。このモデルはAPIから削除できません。
BUILT_IN_OLLAMA_MODEL=qwen3:0.6b

# --- API server settings ---
HOST_BIND_IP=127.0.0.1
HOST_PORT=8000
# Number of uvicorn worker processes
NUM_OF_UVICORN_WORKERS=4

# --- Ollama concurrency settings ---
# Maximum simultaneous API requests
CONCURRENT_REQUEST_LIMIT=2
# Maximum parallel processes inside Ollama
MAX_PARALLEL_PROCESSES=2
# Maximum number of models loaded in VRAM
OLLAMA_MAX_LOADED_MODELS=1

# --- Ollama settings ---
# OLLAMA_HOST: URL to Ollama service 
# Default: http://ollama:11434 (container Ollama - works in CI/dev/test)
# For production: change to external Ollama server (e.g., http://your-ollama-server:11434)
# For development with host Ollama: use http://host.docker.internal:11434
OLLAMA_HOST=http://ollama:11434

# --- Database settings ---
POSTGRES_USER=user
POSTGRES_PASSWORD=password
POSTGRES_DB_NAME=olm-api
DATABASE_URL=postgresql+psycopg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB_NAME}
